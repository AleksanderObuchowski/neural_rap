{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "import codecs"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {},
   "source": [
    "import numpy\n",
    "from keras.models import Sequential\n",
    "from keras.layers import Dense\n",
    "from keras.layers import Dropout\n",
    "from keras.layers import LSTM\n",
    "from keras.callbacks import ModelCheckpoint\n",
    "from keras.utils import np_utils"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {},
   "source": [
    "import numpy\n",
    "from keras.models import Sequential\n",
    "from keras.layers import Dense\n",
    "from keras.layers import Dropout\n",
    "from keras.layers import LSTM\n",
    "from keras.callbacks import ModelCheckpoint\n",
    "from keras.utils import np_utils"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "raw",
   "metadata": {},
   "source": [
    "import numpy\n",
    "from keras.models import Sequential\n",
    "from keras.layers import Dense\n",
    "from keras.layers import Dropout\n",
    "from keras.layers import LSTM\n",
    "from keras.callbacks import ModelCheckpoint\n",
    "from keras.utils import np_utils"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [],
   "source": [
    "filename = \"rap_lyrics.txt\"\n",
    "raw_text = codecs.open(filename).read()[:1000000]\n",
    "raw_text = raw_text.lower()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# create mapping of unique chars to integers\n",
    "chars = sorted(list(set(raw_text)))\n",
    "char_to_int = dict((c, i) for i, c in enumerate(chars))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['\\n',\n",
       " ' ',\n",
       " '!',\n",
       " '\"',\n",
       " \"'\",\n",
       " '(',\n",
       " ')',\n",
       " '*',\n",
       " ',',\n",
       " '-',\n",
       " '.',\n",
       " '/',\n",
       " '0',\n",
       " '1',\n",
       " '2',\n",
       " '3',\n",
       " '4',\n",
       " '5',\n",
       " '6',\n",
       " '8',\n",
       " ':',\n",
       " ';',\n",
       " '?',\n",
       " '[',\n",
       " '\\\\',\n",
       " ']',\n",
       " '`',\n",
       " 'a',\n",
       " 'b',\n",
       " 'c',\n",
       " 'd',\n",
       " 'e',\n",
       " 'f',\n",
       " 'g',\n",
       " 'h',\n",
       " 'i',\n",
       " 'j',\n",
       " 'k',\n",
       " 'l',\n",
       " 'm',\n",
       " 'n',\n",
       " 'o',\n",
       " 'p',\n",
       " 'q',\n",
       " 'r',\n",
       " 's',\n",
       " 't',\n",
       " 'u',\n",
       " 'v',\n",
       " 'w',\n",
       " 'x',\n",
       " 'y',\n",
       " 'z',\n",
       " 'ó',\n",
       " 'ą',\n",
       " 'ć',\n",
       " 'ę',\n",
       " 'ł',\n",
       " 'ń',\n",
       " 'ś',\n",
       " 'ź',\n",
       " 'ż',\n",
       " '–',\n",
       " '‘',\n",
       " '’',\n",
       " '…']"
      ]
     },
     "execution_count": 42,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "chars[:100]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Total Characters:  1000000\n",
      "Total Vocab:  66\n"
     ]
    }
   ],
   "source": [
    "n_chars = len(raw_text)\n",
    "n_vocab = len(chars)\n",
    "print(\"Total Characters: \", n_chars)\n",
    "print(\"Total Vocab: \", n_vocab)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Total Patterns:  999900\n"
     ]
    }
   ],
   "source": [
    "# prepare the dataset of input to output pairs encoded as integers\n",
    "seq_length = 100\n",
    "dataX = []\n",
    "dataY = []\n",
    "for i in range(0, n_chars - seq_length, 1):\n",
    "    seq_in = raw_text[i:i + seq_length]\n",
    "    seq_out = raw_text[i + seq_length]\n",
    "    dataX.append([char_to_int[char] for char in seq_in])\n",
    "    dataY.append(char_to_int[seq_out])\n",
    "n_patterns = len(dataX)\n",
    "print(\"Total Patterns: \", n_patterns)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [],
   "source": [
    "# reshape X to be [samples, time steps, features]\n",
    "X = numpy.reshape(dataX, (n_patterns, seq_length, 1))\n",
    "# normalize\n",
    "X = X / float(n_vocab)\n",
    "# one hot encode the output variable\n",
    "y = np_utils.to_categorical(dataY)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {},
   "outputs": [],
   "source": [
    "# define the LSTM model\n",
    "model = Sequential()\n",
    "model.add(LSTM(256, input_shape=(X.shape[1], X.shape[2])))\n",
    "model.add(Dropout(0.2))\n",
    "model.add(Dense(y.shape[1], activation='softmax'))\n",
    "model.compile(loss='categorical_crossentropy', optimizer='adam')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {},
   "outputs": [],
   "source": [
    "# define the checkpoint\n",
    "filepath=\"weights-improvement-{epoch:02d}-{loss:.4f}.hdf5\"\n",
    "checkpoint = ModelCheckpoint(filepath, monitor='loss', verbose=1, save_best_only=True, mode='min')\n",
    "callbacks_list = [checkpoint]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/20\n",
      "999900/999900 [==============================] - 2671s 3ms/step - loss: 2.7430\n",
      "\n",
      "Epoch 00001: loss improved from 2.98728 to 2.74304, saving model to weights-improvement-01-2.7430.hdf5\n",
      "Epoch 2/20\n",
      "999900/999900 [==============================] - 2570s 3ms/step - loss: 2.4246\n",
      "\n",
      "Epoch 00002: loss improved from 2.74304 to 2.42455, saving model to weights-improvement-02-2.4246.hdf5\n",
      "Epoch 3/20\n",
      "999900/999900 [==============================] - 2184s 2ms/step - loss: 2.2667\n",
      "\n",
      "Epoch 00003: loss improved from 2.42455 to 2.26671, saving model to weights-improvement-03-2.2667.hdf5\n",
      "Epoch 4/20\n",
      "999900/999900 [==============================] - 2182s 2ms/step - loss: 2.3761\n",
      "\n",
      "Epoch 00004: loss did not improve from 2.26671\n",
      "Epoch 5/20\n",
      "999900/999900 [==============================] - 2181s 2ms/step - loss: 2.4373\n",
      "\n",
      "Epoch 00005: loss did not improve from 2.26671\n",
      "Epoch 6/20\n",
      "999900/999900 [==============================] - 2179s 2ms/step - loss: 2.5279\n",
      "\n",
      "Epoch 00006: loss did not improve from 2.26671\n",
      "Epoch 7/20\n",
      "999900/999900 [==============================] - 2182s 2ms/step - loss: 2.1235\n",
      "\n",
      "Epoch 00007: loss improved from 2.26671 to 2.12349, saving model to weights-improvement-07-2.1235.hdf5\n",
      "Epoch 8/20\n",
      "999900/999900 [==============================] - 2181s 2ms/step - loss: 2.7526\n",
      "\n",
      "Epoch 00008: loss did not improve from 2.12349\n",
      "Epoch 9/20\n",
      "999900/999900 [==============================] - 2180s 2ms/step - loss: 2.5895\n",
      "\n",
      "Epoch 00009: loss did not improve from 2.12349\n",
      "Epoch 10/20\n",
      "999900/999900 [==============================] - 2182s 2ms/step - loss: 2.5020\n",
      "\n",
      "Epoch 00010: loss did not improve from 2.12349\n",
      "Epoch 11/20\n",
      "999900/999900 [==============================] - 2180s 2ms/step - loss: 2.3228\n",
      "\n",
      "Epoch 00011: loss did not improve from 2.12349\n",
      "Epoch 12/20\n",
      "999900/999900 [==============================] - 2182s 2ms/step - loss: 2.0469\n",
      "\n",
      "Epoch 00012: loss improved from 2.12349 to 2.04692, saving model to weights-improvement-12-2.0469.hdf5\n",
      "Epoch 13/20\n",
      "999900/999900 [==============================] - 2182s 2ms/step - loss: 2.1517\n",
      "\n",
      "Epoch 00013: loss did not improve from 2.04692\n",
      "Epoch 14/20\n",
      "999900/999900 [==============================] - 2181s 2ms/step - loss: 2.1318\n",
      "\n",
      "Epoch 00014: loss did not improve from 2.04692\n",
      "Epoch 15/20\n",
      "999900/999900 [==============================] - 2185s 2ms/step - loss: 2.0631\n",
      "\n",
      "Epoch 00015: loss did not improve from 2.04692\n",
      "Epoch 16/20\n",
      "999900/999900 [==============================] - 2183s 2ms/step - loss: 2.1737\n",
      "\n",
      "Epoch 00016: loss did not improve from 2.04692\n",
      "Epoch 17/20\n",
      "999900/999900 [==============================] - 2187s 2ms/step - loss: 1.8400\n",
      "\n",
      "Epoch 00017: loss improved from 2.04692 to 1.83997, saving model to weights-improvement-17-1.8400.hdf5\n",
      "Epoch 18/20\n",
      "999900/999900 [==============================] - 2386s 2ms/step - loss: 1.8384\n",
      "\n",
      "Epoch 00018: loss improved from 1.83997 to 1.83840, saving model to weights-improvement-18-1.8384.hdf5\n",
      "Epoch 19/20\n",
      "999900/999900 [==============================] - 2969s 3ms/step - loss: 1.7681\n",
      "\n",
      "Epoch 00019: loss improved from 1.83840 to 1.76813, saving model to weights-improvement-19-1.7681.hdf5\n",
      "Epoch 20/20\n",
      "999900/999900 [==============================] - 2493s 2ms/step - loss: 2.2661\n",
      "\n",
      "Epoch 00020: loss did not improve from 1.76813\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<keras.callbacks.History at 0x7f89cf9dc7f0>"
      ]
     },
     "execution_count": 53,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model.fit(X, y, epochs=20, batch_size=128, callbacks=callbacks_list)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "metadata": {},
   "outputs": [],
   "source": [
    "# load the network weights\n",
    "filename = \"weights-improvement-19-1.7681.hdf5\"\n",
    "model.load_weights(filename)\n",
    "model.compile(loss='categorical_crossentropy', optimizer='adam')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "metadata": {},
   "outputs": [],
   "source": [
    "int_to_char = dict((i, c) for i, c in enumerate(chars))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Seed:\n",
      "\"  będziesz mieć powody oto co robił tata jak rapował gdy był młody a tu kolejny tato, kolejny baton o \"\n",
      " ci ciewy so ca bi niemie a tozadz na na nie otpor do wiamie  tie paradz  mak troo no die zniem z sisowoke sie za co zię śminne siesz co mo poradie sie postod nie zatmani na sie iiety pa no bi pe pownaty ta sar ta ciesae so jakt wie cada ranom sarazyj pa sarie ta balaky pako pozysajan sarie poai sie casi raje poyta my iietto to jast tak jie po pa sobr  bi ta tie toja ta sierie  c pak jas po pap to siesze  a ta myłaa miem sc mierar ta coerie sora to jest niedy mie wi moee mozesiea ta swam cetyjasz ti so sabazą azy ca kizanolajy zap tam do baje jak sezrz  taraza słana  rzej stapa to zięt nak za mociekty nak pa talawie za to mom tam to mas ta scły nas ta star pazy za ocr m mam ja dsitam s pal wa ciedze sie ta calda, ch ma tae tem ter zo nas ca ba nie nomcę na sa mie oopocz sa po sie mozaro zabacik  c ta moły my wamię, toraj be mi zilmc   mu wamaszo jast nasze, to za sie wes ma ta  nie  aa jasteś ta kooni  o tycze cię diasno  tar tan jap co jedi w parierie   to pamstwo jest tasze, tesz mas\n",
      "Done.\n"
     ]
    }
   ],
   "source": [
    "import sys\n",
    "# pick a random seed\n",
    "start = numpy.random.randint(0, len(dataX)-1)\n",
    "pattern = dataX[start]\n",
    "print (\"Seed:\")\n",
    "print (\"\\\"\", ''.join([int_to_char[value] for value in pattern]), \"\\\"\")\n",
    "# generate characters\n",
    "for i in range(1000):\n",
    "\tx = numpy.reshape(pattern, (1, len(pattern), 1))\n",
    "\tx = x / float(n_vocab)\n",
    "\tprediction = model.predict(x, verbose=0)\n",
    "\tindex = numpy.argmax(prediction)\n",
    "\tresult = int_to_char[index]\n",
    "\tseq_in = [int_to_char[value] for value in pattern]\n",
    "\tsys.stdout.write(result)\n",
    "\tpattern.append(index)\n",
    "\tpattern = pattern[1:len(pattern)]\n",
    "print(\"\\nDone.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "'Python Interactive'",
   "language": "python",
   "name": "5eb05ca4-fefb-457b-9646-57eda48ba0b1"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
